# Dockerfile.airflow

# 1) Use the official Airflow image with Python 3.10
FROM apache/airflow:2.7.0-python3.10

# 2) Switch to root to copy files and set permissions
USER root

# 3) Copy your combined requirements.txt (includes both Airflow & training dependencies)
COPY training/requirements.txt /requirements.txt

# 4) Ensure the file is owned by 'airflow' so pip can run as that user
RUN chown airflow: /requirements.txt

# 5) Switch to the 'airflow' user to install Python packages
USER airflow

# 6) Install all Python packages (train_models.py, preprocessing, etc.) under airflowâ€™s home
RUN pip install --no-cache-dir -r /requirements.txt

# 7) Switch back to root to copy code and fix file permissions
USER root

# 8) Copy your entire 'training' directory into /opt/airflow/dags/training/
COPY training/ /opt/airflow/dags/training/

# 9) Copy DAG files into Airflow's DAG folder root
COPY dags/ /opt/airflow/dags/

# 10) Ensure the airflow user owns these new files and directories
RUN chown -R airflow: /opt/airflow/dags

# 11) Set PYTHONPATH to include the DAGs directory
ENV PYTHONPATH="${PYTHONPATH}:/opt/airflow/dags"

# 12) Switch back to the airflow user for runtime
USER airflow

# 13) No CMD/ENTRYPOINT here, since docker-compose.yml will launch Scheduler & Webserver
